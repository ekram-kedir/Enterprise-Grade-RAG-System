{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 : Prompt Testing and Ranking\n",
    "Goals\n",
    "- Comprehensive Evaluation: Provide a robust system that uses various methodologies for a thorough assessment of prompts.\n",
    "- Customizable and User-Centric: Allow users to choose or customize their preferred evaluation methods.\n",
    "- Dynamic and Adaptive: Ensure the system remains flexible and adaptive, capable of incorporating new ranking methodologies as they emerge.\n",
    "\n",
    "# Primary Methods\n",
    "\n",
    "- Monte Carlo Matchmaking: This method is used to select and match different prompt candidates against each other. The Monte Carlo method, known for its applications in problem-solving and decision-making processes, helps in optimizing the information gained from each prompt battle. By simulating various matchups, it allows the system to test the effectiveness of each prompt in different scenarios.\n",
    "- ELO Rating System:  This system, which is commonly used in chess and other competitive games, rates the prompts based on their performance in the battles. Each prompt candidate is assigned a rating that reflects its success in previous matchups. The system takes into account not just the number of wins but also the  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "rpath = os.path.abspath('./..')\n",
    "if rpath not in sys.path:\n",
    "    sys.path.insert(0, rpath)\n",
    "    \n",
    "from utility.testing_and_ranking import evaluate_prompt, elo_ratings_func\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conduct multiple rounds of evaluation\n",
    "- Sort prompts by their final Elo ratings and then Print the ranked prompts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are some key areas of knowledge acquisition for prompt engineering?: 1548.7888289685131\n",
      "Which companies are doing something similar to this project?: 1524.186472437636\n",
      "What are some techniques to improve RAG in RAG?: 1523.9456928737486\n",
      "What are the key performance indicators for understanding the challenge?: 1522.5243266611412\n",
      "What are the tasks involved in developing the prompt generation system?: 1456.5759865967193\n"
     ]
    }
   ],
   "source": [
    "questions = ['What are the key performance indicators for understanding the challenge?', \n",
    "          'What are some techniques to improve RAG in RAG?', \n",
    "          'What are some key areas of knowledge acquisition for prompt engineering?', \n",
    "          'Which companies are doing something similar to this project?', \n",
    "          'What are the tasks involved in developing the prompt generation system?']\n",
    "\n",
    "elo_ratings = {prompt: 1500 for prompt in questions}  # Initial ratings\n",
    "\n",
    "for _ in range(10):  # Number of rounds\n",
    "    elo_ratings = elo_ratings_func(questions, elo_ratings)\n",
    "\n",
    "sorted_prompts = sorted(questions, key=lambda x: elo_ratings[x], reverse=True)\n",
    "\n",
    "for prompt in sorted_prompts:\n",
    "    print(f\"{prompt}: {elo_ratings[prompt]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'main_prompt': {'Monte Carlo Evaluation': 1.97, 'Elo Rating Evaluation': 1504.2019499940866}, 'test_case_1': {'Monte Carlo Evaluation': 2.04, 'Elo Rating Evaluation': 1489.2019499940866}, 'test_case_2': {'Monte Carlo Evaluation': 2.0, 'Elo Rating Evaluation': 1519.2019499940866}, 'test_case_3': {'Monte Carlo Evaluation': 2.04, 'Elo Rating Evaluation': 1519.2019499940866}, 'test_case_4': {'Monte Carlo Evaluation': 1.87, 'Elo Rating Evaluation': 1504.2019499940866}, 'test_case_5': {'Monte Carlo Evaluation': 2.25, 'Elo Rating Evaluation': 1504.2019499940866}}\n"
     ]
    }
   ],
   "source": [
    "main_prompt = 'What are the key performance indicators for understanding the challenge?'\n",
    "test_cases = ['What are the key performance indicators for understanding the challenge?', \n",
    "          'What are some techniques to improve RAG in RAG?', \n",
    "          'What are some key areas of knowledge acquisition for prompt engineering?', \n",
    "          'Which companies are doing something similar to this project?', \n",
    "          'What are the tasks involved in developing the prompt generation system?']\n",
    "result = evaluate_prompt(main_prompt, test_cases)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
